[package]
name = "bitnet-inference"
version = "0.1.1"
edition.workspace = true
authors.workspace = true
license.workspace = true
repository.workspace = true
homepage.workspace = true
documentation.workspace = true
description = "High-performance inference engine for BitNet models"
keywords = ["bitnet", "inference", "neural-networks", "text-generation"]
categories = ["science"]

[features]
default = ["std", "tokenizers"]
std = []
# Enable tokenization
tokenizers = ["dep:tokenizers"]
# Enable text generation utilities
generation = ["dep:rand"]
# Enable batch processing
batching = ["dep:rayon"]
# Enable streaming inference
streaming = ["dep:tokio"]
# Enable Metal acceleration (temporarily disabled due to dependency issues)
# metal = ["dep:bitnet-metal"]
# Enable MLX acceleration for Apple Silicon
mlx = ["dep:mlx-rs"]
# Enable MLX-accelerated inference
mlx-inference = ["mlx", "generation"]
# Enable Apple Silicon optimizations (MLX + Metal when available)
apple-silicon = ["mlx"]

[dependencies]
bitnet-core = { path = "../bitnet-core", version = ">=0.1.0, <0.4.0" }
bitnet-quant = { path = "../bitnet-quant", version = ">=0.1.0, <0.3.0" }
# bitnet-metal = { path = "../bitnet-metal", version = "0.1.0", optional = true }

# Core dependencies
candle-core.workspace = true
serde.workspace = true
anyhow.workspace = true
thiserror.workspace = true

# Tokenization
tokenizers = { workspace = true, optional = true }

# Text generation
rand = { version = "0.8", optional = true }

# Performance
rayon = { workspace = true, optional = true }
tokio = { workspace = true, optional = true }

# MLX acceleration
mlx-rs = { workspace = true, optional = true }

# Utilities
tracing = { workspace = true, optional = true }
uuid = "1.0"

[dev-dependencies]
criterion.workspace = true
proptest.workspace = true
tokio = { workspace = true, features = ["test-util"] }
